{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\aparna\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\aparna\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aparna\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers sentencepiece requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from sacremoses) (0.17.0)\n",
      "Requirement already satisfied: regex in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from sacremoses) (2020.10.15)\n",
      "Requirement already satisfied: click in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from sacremoses) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from sacremoses) (4.67.1)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\aparna\\anaconda3\\lib\\site-packages (from tqdm->sacremoses) (0.4.4)\n",
      "Installing collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aparna\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aparna\\.cache\\huggingface\\hub\\models--facebook--nllb-200-distilled-600M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Paragraph (100 words):\n",
      "The importance of education in society cannot be overstated. Education is the basic part of a family, which is why I love having her. We need a teacher who is passionate in her passion for teaching and who loves all of us. She is part and parcel of every woman's education and every person's mother-in-law's child-care experience. I have always believed in the importance and the pride that women feel and thrive as mothers. It is that sense of pride and joy that makes us proud and make us think about our mothers, her talents and her aspirations. I want\n",
      "\n",
      "Total words: 100\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Define a prompt to guide the generation\n",
    "prompt = \"The importance of education in society cannot be overstated. Education\"\n",
    "\n",
    "# Generate a paragraph of approximately 100 words\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_length=150,  # Adjust max_length to get close to 100 words\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Extract the generated text\n",
    "generated_text = output[0]['generated_text']\n",
    "\n",
    "# Ensure the paragraph is exactly 100 words\n",
    "words = generated_text.split()[:100]\n",
    "paragraph = ' '.join(words)\n",
    "\n",
    "# Print the result\n",
    "print(\"Generated Paragraph (100 words):\")\n",
    "print(paragraph)\n",
    "print(f\"\\nTotal words: {len(paragraph.split())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation and comparison completed. Check the output files:\n",
      "- original_text.txt\n",
      "- translated_text_1.txt\n",
      "- translated_text_2.txt\n",
      "- difference.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from transformers import pipeline\n",
    "import difflib\n",
    "# Ensure the text is exactly 100 words\n",
    "sample_text=paragraph\n",
    "#sample_text = ' '.join(sample_text.split()[:100])\n",
    "\n",
    "# Save original text to a file\n",
    "with open(\"original_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# Initialize the first translation model (Helsinki-NLP/opus-mt-en-es)\n",
    "translator1 = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "# Initialize the second translation model (Facebook NLLB-200)\n",
    "translator2 = pipeline(\"translation\", model=\"facebook/nllb-200-distilled-600M\", tokenizer=\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "# Step 2: Translate using the first model (Helsinki-NLP/opus-mt-en-es)\n",
    "translated_spanish_1 = translator1(sample_text)[0]['translation_text']\n",
    "\n",
    "# Save first translated text to a file\n",
    "with open(\"translated_text_1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(translated_spanish_1)\n",
    "\n",
    "# Step 3: Translate using the second model (Facebook NLLB-200)\n",
    "translated_spanish_2 = translator2(sample_text, src_lang=\"eng_Latn\", tgt_lang=\"spa_Latn\")[0]['translation_text']\n",
    "\n",
    "# Save second translated text to a file\n",
    "with open(\"translated_text_2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(translated_spanish_2)\n",
    "\n",
    "# Step 4: Compute differences between the two translated texts\n",
    "diff = list(difflib.unified_diff(translated_spanish_1.split(), translated_spanish_2.split(), \n",
    "                                 fromfile=\"Helsinki-NLP_Opus\", tofile=\"Facebook_NLLB\", lineterm=''))\n",
    "\n",
    "# Save the difference to a file\n",
    "with open(\"difference.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(diff))\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Translation and comparison completed. Check the output files:\")\n",
    "print(\"- original_text.txt\")\n",
    "print(\"- translated_text_1.txt\")\n",
    "print(\"- translated_text_2.txt\")\n",
    "print(\"- difference.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--- Helsinki-NLP_Opus',\n",
       " '+++ Facebook_NLLB',\n",
       " '@@ -20,15 +20,18 @@',\n",
       " ' una',\n",
       " ' familia,',\n",
       " ' por',\n",
       " '-eso',\n",
       " '+lo',\n",
       " '+que',\n",
       " ' me',\n",
       " ' encanta',\n",
       " ' tenerla.',\n",
       " ' Necesitamos',\n",
       " ' una',\n",
       " ' maestra',\n",
       " '+que',\n",
       " '+sea',\n",
       " ' apasionada',\n",
       " '-en',\n",
       " '+por',\n",
       " ' su',\n",
       " ' pasión',\n",
       " ' por',\n",
       " '@@ -37,11 +40,10 @@',\n",
       " ' y',\n",
       " ' que',\n",
       " ' nos',\n",
       " '-ama',\n",
       " '+ame',\n",
       " ' a',\n",
       " ' todos.',\n",
       " '-Ella',\n",
       " '-es',\n",
       " '+Es',\n",
       " ' parte',\n",
       " ' integral',\n",
       " ' de',\n",
       " '@@ -86,10 +88,10 @@',\n",
       " ' orgullo',\n",
       " ' y',\n",
       " ' alegría',\n",
       " '+el',\n",
       " ' que',\n",
       " ' nos',\n",
       " ' hace',\n",
       " '-sentir',\n",
       " ' orgullosos',\n",
       " ' y',\n",
       " ' nos',\n",
       " '@@ -103,3 +105,54 @@',\n",
       " ' y',\n",
       " ' sus',\n",
       " ' aspiraciones.',\n",
       " '+Quiero',\n",
       " '+que',\n",
       " '+la',\n",
       " '+gente',\n",
       " '+se',\n",
       " '+sienta',\n",
       " '+orgullosa',\n",
       " '+de',\n",
       " '+ella',\n",
       " '+y',\n",
       " '+que',\n",
       " '+la',\n",
       " '+familia',\n",
       " '+se',\n",
       " '+sienta',\n",
       " '+orgullosa',\n",
       " '+de',\n",
       " '+ella',\n",
       " '+y',\n",
       " '+que',\n",
       " '+la',\n",
       " '+familia',\n",
       " '+se',\n",
       " '+sienta',\n",
       " '+feliz',\n",
       " '+de',\n",
       " '+ella',\n",
       " '+y',\n",
       " '+que',\n",
       " '+la',\n",
       " '+familia',\n",
       " '+se',\n",
       " '+sienta',\n",
       " '+feliz',\n",
       " '+de',\n",
       " '+ella',\n",
       " '+y',\n",
       " '+que',\n",
       " '+la',\n",
       " '+familia',\n",
       " '+se',\n",
       " '+sienta',\n",
       " '+feliz',\n",
       " '+de',\n",
       " '+ella',\n",
       " '+y',\n",
       " '+que',\n",
       " '+la',\n",
       " '+familia',\n",
       " '+se',\n",
       " '+sienta']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
